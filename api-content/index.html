{"posts":[{"title":"2020/1/21","content":"分开定义softmax运算和交叉熵损失函数可能会造成数值不稳定 摘自:https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.7_softmax-regression-pytorch?id=_373-softmax和交叉熵损失函数 暂未解决,后续查资料了解下 先写下softmax函数的公式,假设我们有一数组V,ViV_{i}Vi​表示V中第i个元素,那么该元素的softmax值是: Si=ei∑jejS_{i}=\\frac{e^{i}}{\\sum_{j}^{}e^{j} } Si​=∑j​ejei​ 其中原mxnet版本,沐神给的问题是: 3.6.10. 练习 - 在本节中，我们直接按照softmax运算的数学定义来实现softmax函数。这可能会造成什么问题？（提示：试一试计算 exp(50) 的大小。） - 本节中的cross_entropy函数是按照“softmax回归”一节中的交叉熵损失函数的数学定义实现的。这样的实现方式可能有什么问题？（提示：思考一下对数函数的定义域。） - 你能想到哪些办法来解决上面的两个问题？ 第一个问题很明显,exp(50)数值计算溢出(即x较大),为了解决这个问题的方法就是改变x的值咯,搬运知乎一份回答:https://zhuanlan.zhihu.com/p/27223959 公式变换: def softmax(X): X = X-X.max() exp_X = X.exp() return exp_X/exp_X.sum() 即通过将每个ViV_{i}Vi​值减去数组V中最大值来起到降低exp()函数输入的作用,解决问题 第二个问题先看这个交叉熵函数,这里摘录动手学深度学习pytorch版本2.5节的代码,并非mxnet版本 def cross_entropy(y_hat, y): return - torch.log(y_hat.gather(1, y.view(-1, 1))) #其中数据为: y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y = torch.LongTensor([0, 2]) y.view(-1,1)使用了pytorch中的torch.view()函数,用来将y变换形态,其中的参数-1代表这个位置数值由其他值推断,所以应该得到 print(y.view(-1,1)) tensor([[0], [2]]) 理一下上面的,y_hat是两个数据对应3个类别的概率图,y是两个数据对应的类别标签,所以我们用gather取出对应类别的概率,输出为: tensor([[0.1000], [0.5000]]) 参考https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.4_softmax-regression?id=_345-交叉熵损失函数 写的交叉熵函数,此处的yjy_{j}yj​非1即0,所以我们的函数就被简化成−log(y^i)-log(\\hat{y}_{i})−log(y^​i​)了 思考对数定义域为(0, -&amp;),但是因为输出y^\\hat{y}y^​是softmax输出不应该存在负数(指数函数),应考虑点是如果y^\\hat{y}y^​输出过小时无限接近于0,该交叉熵函数即负对数函数结果值过大为nan 可以考虑在交叉熵中添加一极小值避免情况 以上全文参考文献: 请问练习里的这两个问题有什么解决方案？我实在没想到，因为第一个exp(50)好大好大，第二个问题，ln定义域必须大于0，但根本不可能小于0啊，因为损失函数出来的值根本不会小于等于0啊 对于分开定义不稳定的解决方法 虽然这句话有点nt,我们用一个包括两者的函数即可,数值稳定性会更好 pytorch中,这个函数称为:nn.CrossEntropyLoss() 该损失函数结合了nn.LogSoftmax()和nn.NLLLoss()两个函数,它在做分类（具体几类）训练的时候是非常有用的。 torch官方文档:CROSSENTROPYLOSS 激活函数,yyds 从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。 上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。 摘自:https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.8_mlp?id=_382-激活函数 CS231n讲解权重初始化 CS231n课程笔记翻译：神经网络笔记 2 如何理解训练误差和泛化误差(一个极好的例子,沐神666) 摘录链接:3.11.1 训练误差和泛化误差 在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。 让我们以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。假设训练题和测试题都随机采样于一个未知的依照相同考纲的巨大试题库。如果让一名未学习中学知识的小学生去答题，那么测试题和训练题的答题错误率可能很相近。但如果换成一名反复练习训练题的高三备考生答题，即使在训练题上做到了错误率为0，也不代表真实的高考成绩会如此。 在机器学习里，我们通常假设训练数据集（训练题）和测试数据集（测试题）里的每一个样本都是从同一个概率分布中相互独立地生成的。基于该独立同分布假设，给定任意一个机器学习模型（含参数），它的训练误差的期望和泛化误差都是一样的。例如，如果我们将模型参数设成随机值（小学生），那么训练误差和泛化误差会非常相近。但我们从前面几节中已经了解到，模型的参数是通过在训练数据集上训练模型而学习出的，参数的选择依据了最小化训练误差（高三备考生）。所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。 机器学习模型应关注降低泛化误差。 csv数据预处理 来源是&lt;动手学&gt;的房价预测,一直在做图像处理,还没怎么见过csv格式的处理,学习一下 3.16.3 预处理数据 ","link":"https://mbkotori.github.io/post/2020121/"},{"title":"2021/1/20","content":"表示图片时对格式的注意 注意： 由于像素值为0到255的整数，所以刚好是uint8所能表示的范围，包括transforms.ToTensor()在内的一些关于图片的函数就默认输入的是uint8型，若不是，可能不会报错但可能得不到想要的结果。所以，如果用像素值(0-255整数)表示图片数据，那么一律将其类型设置成uint8，避免不必要的bug。 本人就被这点坑过，详见我的这个博客2.2.4节。 以上摘自https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.5_fashion-mnist?id=_351-获取数据集. 什么是模型微调 CNN入门讲解：什么是微调（Fine Tune）？ pytorch中的gather和scatter函数 先看看gather: gather,单词中文释义为:聚集,集合 官方文档: torch.gather(input, dim, index, out=None) → Tensor Gathers values along an axis specified by dim. For a 3-D tensor the output is specified by: out[i][j][k] = input[index[i][j][k]][j][k] # dim=0 out[i][j][k] = input[i][index[i][j][k]][k] # dim=1 out[i][j][k] = input[i][j][index[i][j][k]] # dim=2 Parameters: input (Tensor) – The source tensor dim (int) – The axis along which to index index (LongTensor) – The indices of elements to gather out (Tensor, optional) – Destination tensor Example: &gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]]) &gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]])) 1 1 4 3 [torch.FloatTensor of size 2x2] 可以看到,gather是根据我们给的索引index在input中数据得到out,其中除了index外输出还会受到dim的影响,下面用张图直观表示下 再看看scatter: scatter中文释义:撒,撒播,散开 这个函数则是将数据根据索引回填到一个新矩阵中,根据它的功能我们很适合拿他来做onehot矩阵 同样应有输入input,操作维度dim,索引index,我们同样利用一张图片李姐: 以下列出几个参考网站,方便读者进行进一步阅读理解: Pytorch的gather和scatter 3分钟理解 pytorch 的 gather 和 scatter torch.gather/scatter_使用 一个没看懂的代码 def accuracy(y_hat, y): return (y_hat.argmax(dim=1) == y).float().mean().item() 用来计算y_hat和y的准确率 y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]) y = torch.LongTensor([0, 2]) 理一下思路,首先其中y_hat.argmax(dim=1)返回矩阵y_hat每行中最大元素的索引，且返回结果与变量y形状相同。 即print(y_hat.argmax(dim=1)) = tensor([2,2]) 然后做相等判断式,和y进行比较,判断式输出为:[False, true],是一个类型为ByteTensor的Tensor，我们用float()将其转换为值为0（相等为假）或1（相等为真）的浮点型Tensor。 也就是到此处为:print((y_hat.argmax(dim=1) == y).float()) = tensor([0., 1.]) 注:ByteTensor在c语言中对应的话,可以看作无符号字符串 对tensor([0., 1.])做torch.mean()操作,默认不设置dim时返回所有元素的平均值,设置dim后则按维度给返回值 此处输出则为(0+1)/2=0.5. print(accuracy(y_hat, y)) 输出: 0.5 最后结尾的item()用法则是得到一个元素张量里面的元素值,如果不使用item,输出应为: tensor(0.5000) ","link":"https://mbkotori.github.io/post/2021120/"},{"title":"2021/1/19","content":"今天更了一篇新文章 其实一开始只是想研究下latex写在每日总结里,结果稀里糊涂连着换了主题,忙了半下午,索性弄了一篇文章出来 关于损失函数(平方损失和交叉熵) 进行预测概率时(分类),选择平方损失函数就过于严格,例如y1=y2=0.2和y1=0,y2=0.4的损失就差距很大,但是两者的分类结果相同 换用交叉熵的话,根据公式他将只关心正确类别的概率,更为合理 来源:https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.4_softmax-regression?id=_345-交叉熵损失函数 ","link":"https://mbkotori.github.io/post/2021119/"},{"title":"换了个主题，顺便研究下在md中的latex方法","content":"在线LaTeX公式编辑器： https://www.latexlive.com/ 写法如下： $$O^{i} =x^{i}W+b$$ 这个latex方法似乎不是markdown自带的,如果要实现latex似乎需要使用到js渲染(不知道说的对不对),总之按gridea看来得在主题文件中包含,本人还没有修改主题的能力,只能将手头的主题重新尝试了下选定了一个 之后按这个顺序做了下测试,看看具体写作方法:(怪捞的) 看看$$O^{i} =x^{i}W+b$$ 看看$O^{i} =x^{i}W+b$ $$O^{i} =x^{i}W+b$$ $O^{i} =x^{i}W+b$ 在gridea的本地渲染是这样的: 下面是为了测试看主题带的渲染和gridea本地是否相同写下的四行: 看看$$O^{i} =x^{i}W+b$$ 看看Oi=xiW+bO^{i} =x^{i}W+bOi=xiW+b Oi=xiW+bO^{i} =x^{i}W+b Oi=xiW+b Oi=xiW+bO^{i} =x^{i}W+bOi=xiW+b 由此看出,双美元符号$$是行间公式,单美元符号$是行内公式 另一个有趣的是在符号之间输入\\时是会有latex关键字提示的,很方便 另一种方法,利用知乎渲染的api进行公式书写 不过这样似乎就变成html语句了,且输出公式时图片一张,不方便行内书写和格式调整 书写方法和展示结果如下: &lt;img src=&quot;https://www.zhihu.com/equation?tex=O^{i} =x^{i}W+b&quot; /&gt; ","link":"https://mbkotori.github.io/post/huan-liao-ge-zhu-ti-shun-bian-yan-jiu-xia-zai-md-zhong-de-latex-fang-fa/"},{"title":"2021/1/18","content":"pytorch搭建网络（定义模型） （1）导入torch.nn模块，nn就是利用autograd定义模型，nn的核心数据结构是Module，这是一个抽象概念，可以表示神经网络中某个词或者包含多层的神经网络。 实际运用时，最常见做法是继承nn.Module，一个nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法 这部分以后我得好好理清楚面向对象的部分 个人理解：nn.Module就是标准积木，现在我们的模型就是某个我们设计好的模型玩具，要想搭成这个玩具，我们先得从标准积木（nn.Module）中构建出我们要的指定模块(网络层)，最后forward方法就是自己搞出来的‘拼装手册’，怎么把我们的积木拼成我们的玩具。 想想8x8微型积木就好了 当然，简单方法就是nn.Sequential，这个最方便，直接net=nn.Sequential（网络层），网络层按你传入的顺序添加到计算图中 可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器。 for param in net.parameters(): print(param) 给不同子网络设置不同学习率（finetune常用） 举例： optimizer =optim.SGD([ # 如果对某个参数不指定学习率，就使用最外层的默认学习率 {'params': net.subnet1.parameters()}, # lr=0.03 {'params': net.subnet2.parameters(), 'lr': 0.01} ], lr=0.03) 似乎在pytorch中，也有类似的方法用于将多个子网络训练，之后可以进行研究 torch中的各种轮子（深度学习框架就是这么用的嘛） torch.utils.data模块提供了有关数据处理的工具，torch.nn模块定义了大量神经网络的层，torch.nn.init模块定义了各种初始化方法，torch.optim模块提供了很多常用的优化算法。 Softmax特点 softmax运算常被用在分类问题，他的好处就是将我们多分类的输出值变换成正值且和为一的概率分布 之后我们可以通过取最大概率作为分类判断依据 参考网页：https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.4_softmax-regression?id=_342-softmax回归模型 ","link":"https://mbkotori.github.io/post/2021118/"},{"title":"2021/1/16","content":"tensor和numpy转换 使用numpy()和from_numpy()相互转换,这两函数生成结果和源数据是共享内存的(速度快,但是改变一个另一个也改变) 另一方法是torch.tensor(),该方法是先数据拷贝,时间空间消耗大,也不共享内存 除了CharTensor外,所有在CPU上的tensor都可以和numpy数组转换 (感想:自由啊,回想起tensorflow中连print都打印不出内容的情况,pytorch某种程度上方便太多了!) 用to()将tensor在GPU,CPU上移动,当然你得有gpu 指定device: device = torch.device('cuda') 移动: x=x.to(device) 这就把实现了一次移动 autograd 这个包能根据输入和前向传播自动构建计算图和执行反向传播 核心类:tensor 将属性.requires_grad设置True就会追踪tensor的操作(那么就可以利用链式法则进行梯度传播),调用.backward()完成梯度计算,tensor的梯度会积累到.grad属性中 如果要取消追踪,就要调用.detach()从追踪记录分离,也可以使用with torch.no_grad()将不想追踪的代码包裹起来,这在评估模型(eval)时很有用 补充资料:PyTorch 的 backward 为什么有一个 grad_variables 参数？ https://zhuanlan.zhihu.com/p/29923090 我觉得这部分我看的是挺头晕的,这部分的逻辑还是要再深入看看的 什么是超参数 需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。 来源:https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3-优化算法 之后有机会可以详细写一篇关于超参数的整合文章 计算代码运行时间 start = time() 应该运行的代码块 print(time() - start) python的yield关键字 起因是看到这样一段代码： 来源是：https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.2_linear-regression-scratch?id=_322-读取数据 # 本函数已保存在d2lzh包中方便以后使用 def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # 样本的读取顺序是随机的 for i in range(0, num_examples, batch_size): j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch yield features.index_select(0, j), labels.index_select(0, j) 这是一个简单的dataloader函数，但是在最后返回值上，我们可以看到他并没使用return，而是用了yield关键字 关于yield，得讨论到生成器generator，他是记录一定的算法规则，和一般的迭代器不同的是，他是在需要调用时再根据算法推算出相应元素，而不用一口气算完 yield类似return，回想一下return，函数中执行到这个关键字就会返回响应结果并终止函数执行 当yield被塞入函数里，会发生什么呢-&gt;该函数变成了生成器-&gt;我们能用next使用它或遍历 考虑到生成器的目的：记录一定算法规则得到按需生成的列表 所以yield一般在for循环语句以一定规则生成，之后调用函数，得到一个生成器，再遍历生成器使用它 当然，yield也可以用在别的地方，生成器只是一种使用方法 就像打电玩一样，你蓄力发大招的时候，如果执行了return，就直接把大招发出去了，蓄力结束 如果执行了yield，就相当于返回一个生成器对象，每调用一次next（），就发一个大招（来源知乎） 与return不同，yield在函数中返回值时会保存函数的状态，使下一次调用函数时会从上一次的状态继续执行，即从yield的下一条语句开始执行，这样做有许多好处，比如我们想要生成一个数列，若该数列的存储空间太大，而我们仅仅需要访问前面几个元素，那么yield就派上用场了，它实现了这种一边循环一边计算的机制，节省了存储空间，提高了运行效率。 总结整理我们最开始的这段代码： 我们包装的这么一个dataloader，应该将我们的数据根据需要的batchsize分成一段一段的数据（如1000数据，bs=4，就是250组），然后每次输出一组数据，这时候根据yield会保存到该组数据位置的状态，然后该组进入网络运算过程，实现网络和反向传播后，再循环回来找dataloader要下一组数据，yield的功能就体现出来了，因为我们从直接的状态就可以直接读下一组而不是从头按照索引找新的一组数据。这样 注：在pytorch中，我们的dataloader库也使用了类似这样的方法 ","link":"https://mbkotori.github.io/post/2021116/"},{"title":"2021/1/11","content":"1.pytorch各个损失函数都有自己的前提，有时候不能混用， 举例：CrossEntropyLoss，该函数进行验证时label必须是以0开始的，即 classes=【0，1，2，3】 否则报错：Assertion `cur_target &gt;= 0 &amp;&amp; cur_target &lt; n_classes’ failed. 2.数据选择转换格式时是：dtype而不是detype，注意拼写 3.多分类时使用np.argmax（），返回numpy数组中最大值的索引，根据索引即可代表网络最后给该像素的类型值 4.代码未完全确认可以使用时，优先构建小数据集来快速验证可行性，不要出现今天这种跑了4k数据才发现eval模块出错的情况 5.softmax和sigmoid： softmax用于多分类，把多个输出映射到（0，1）区间内且这些输出的结果和为1（满足概率的特性） 和上面联动：softmax后再使用argmax来找到某个概率值最大的分类索引，得到判断结果 sigmoid：注意，是sigmoid，不是sigmiod也不是sigmod，容易写错 sigmoid就是逻辑斯蒂函数，即logistic函数，把一个实数映射到（0，1）的区间，拿来做二分类，但是注意，你拿来做多分类也行，只是多分类的值相加，不会像softmax一样各分类总值相加为1，一般来说，拿来二分类还需要给他设置个阈值，将（0，1）的值映射成0/1或者0/255 另一个方法，二分类也是多分类，那你用softmax啊！但是这时候，你的网络输出应该是2层，即最后的out_channel=2，然后argmax取值作为分类，是个好办法 5.1 知乎说法： sigmoid的优点在于输出范围有限，所以数据在传递的过程中不容易发散。当然也有相应的缺点，就是饱和的时候梯度太小。sigmoid还有一个优点是输出范围为(0, 1)，所以可以用作输出层，输出表示概率。评论中 @林孟潇指出了sigmoid的另一个优点：求导容易。 作者：王赟 Maigo 链接：https://www.zhihu.com/question/24259872/answer/82598145 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 ","link":"https://mbkotori.github.io/post/2021111-zong-jie/"},{"title":"Residual Feature Aggregation Network for Image Super-Resolution","content":"翻译名:用于图像超分辨的残差特征聚合网络 abstract: 解决问题-SISR,单张图像超分辨率问题 考虑点-利用残差特征中的 残差分支-分层特征( However, existing methods neglect to fully utilize the hierarchical features on the residual branches.) 提出解决方法-提出残差特征聚合(residual feature aggregation,RFA)框架,将残差块组合在一起,并添加跳过链接[有点像densenet啊这个说法和图];提出增强空间注意力特征(enhanced spatial attention,ESA),使残差更集中于空间特征(?) 最后将RFA和ESA一同运用 introduction: SISR-直接将低分辨率LR图转成高分辨率HR图(单进单出siso),不像一般超分辨会有ms图 {读图figure1} 这里(a)实际上是一个多resblock的前向结构,每个模块包含一个resblock,由四个resblock组成 (b)中描述他们的RFA结构,先通过3个resblock前向图,他们的输出到第四个resblock,之后Res4模块输出和前三个block各自的输出concat到一块,随后1x1卷积来聚集/融合他们的特征,相对于(a)增加了1x1卷积的参数消耗 {读图figure8} 不同残差块输出的特征可以反映空间内容的不同方面 |--考虑思路:用空间注意力来提升空间信息表达能力,增强特征的空间分布 自称工作: 1.提出RFA框架 2.提出加强空间注意力ESA块 3.提出RFANet,基于RFA和ESA模块进行构建 realated work sub-pixel conv:子像素卷积,超分辨率中常用的upscale方法 https://oldpan.me/archives/upsample-convolve-efficient-sub-pixel-convolutional-layers [后续可能会出这篇文章的内容,我还没读但是看起来很有意思,他和deconv方法的不同也可以考虑进去,另一点是他能应用在SR上能不能应用在分割上,做一下考虑] {读图figure2} 基础的SR模型,卷积+多个设计好的模块+反卷积+卷积回复通道数? 基于注意力的网络模型 3.methodology 3.1 SR的简单网络结构 看作三部分: 头部:负责一个卷积层的初始特征提取,得到浅层特征F0 躯干:输入是提取到的特征F0,连接到T个basemodule的复读模式{公式2} 重建restore:将上面提取的(深层特征Ft和浅层特征F0)融合,进行放大[全局残差学习可以减轻训练难度] upsample模块是重建中的关键部分,使用子像素卷积 损失函数用L1 loss(改损失函数或许是一个很好的改进切入点) 3.2 残差特征聚合模块 残差模块包含双分支(1)残差块(2)身份块?(identity branch) 前面的残差块提取的信息很难被利用-&gt;因为它需要很长路径到达最后的卷积运算 -&gt;改进[直接将每个块的残差特征concat到一起] -&gt;concat后用1x1卷积融合他们的特征哦(这个东西可不仅仅是降维减通道) 这样的连接,不用让前面的残差信息受损或干扰-&gt;得到更具有区分性的特征表达(?) RFA可以看作一个独立模块,这也意味着我们可以把它和其他模块连接(模块复读和不同模块组合,哪个效果会好一些呢) 3.3 增强的空间注意力模块 {读图figure4} 左侧是ESAblock的形状,右侧是ESAblock中的ESA机制(mechanism) 仔细看ESA机制,对其做一个解释 ESA块放到残差块的末尾来起作用,注意力机制本质(改天可以再写一篇文章),集中体现残差特征 同时考虑到该ESA块要被连接到你后面的每个残差块,尽量不要做得太臃肿 为了完成图像SR任务,应设计一个具有较大感受野的模块(?此处没有理解.关于感受野下次可以再写一篇) 解决方法也很简单:1x1卷积降维减参数+步长卷积增大感受野 注意这里,描述的是strided conv(步长卷积)而不是dilated conv,这两者存在区别(可以详细写一下) 跨步卷积搭配池化使用#为什么/原因.可以详细描述一下 这里提到的:&quot;抛去计算量考虑,实现空间注意力块的更好方法是使用非本地模块(Non-local block)&quot;我没get到这个部分,之后要详细看一下他这里参考的两篇论文 3.4 实施细节 把RFA和ESA一同使用来构建最终的SR模块(RFANet): |-使用30个RFA模块,每个RFA包含4个ESA模块(从这里就感受到了前文为什么他们说要做一个轻便的ESA模块了) 这里的四个ESA模块也很明显:在每个resblock后,我们都给他插入一个ESA(真的有效吗....保持怀疑) 此处提到的1x1卷积的缩小率(这是个啥?),减速比? reduction ratio 3.5 讨论discussions 将文章网络和MemNet、RDN进行了讨论,描述了他们的优越性 4 实验内容 4.1 设置参数,训练等 数据集来源:DIV2K数据集 数据增强:随机旋转+翻转 batchsize=16,patch_size=48*48 数据集:Set5,Set14,B100,Urban100,Manga109 Bicubic(BI) and blur-downscale (BD) degradation models [36] are used when conducting experiments.(这部分是什么意思?没有理会到) 论文评价： 就我个人而言，这篇文章并没有太大的吸引力：RFA模块类似DenseNet结构，但是又没有很好的去解决密集连接后的参数爆炸（更有甚者他在原网络基础上又增加了ESA模块，就算通过设计使增加的参数不至于过多也是实打实地在每个模块上又添了“浓墨重彩”的一笔支出）。看似巧妙但是仔细琢磨似乎只是对网络的二度修改尝试。 另一点上，消融实验的可解释性确实不高，反而给出了一些漏洞，作者试图用实验解释自己制造的模块的作用，但是弄巧成拙，并没有取得很好的结果。 当然，作为CVPR2020录用的文章，从中我们还是能学习到一些东西的：比如文章中图表制作的合理性（我很喜欢为消融实验所做的table1和计算效率&amp;参数量的figure9，两张图既表达清晰又给人眼前一亮）；还有其中ESA模块的方法，在不能合理解释的情况下尝试他也未尝不可（作者也利用这个模块#######） ","link":"https://mbkotori.github.io/post/residual-feature-aggregation-network-for-image-super-resolution/"},{"title":"关于作者,和这个blog","content":"🏠 关于本站 本站属于个人纪录,分享遇到的坑,鉴于自己即将面对研究生生活,留下这么一个网站来作为自己的知识存储和记录也是非常重要的. 一开始考虑过CSDN or 博客园,考虑到审核问题放弃了,既然自己也没有什么特效需要,索性使用静态blog的形式来制作,也省去域名等麻烦. 👨‍💻 博客内容 主要会记录自己在深度学习方面出现的问题和遇到的坑,也可能分享一些自己的思考感悟(某种情况下可能会超出审核范围,大家辩证阅读即可). 个人更新时间不定,有兴趣的话题可以在留言区提出,有机会整理出来和大家一起分享. 📬 和我联系 本来是想使用Gittalk作为blog的评论系统,但是考虑到需要登陆github还是有点麻烦的,另外gittalk也存在一定的安全隐患.网站也需要一个图床来存图,索性使用了一个折衷的方案.(无利益相关) 网盘既可以作为图床,也有着一个简易的留言板可以作为评论区来供大家联系我,我会定期在上面进行回复,留言区限制1K条信息所以不重要的或者已解决的会删除. 这是 我们的评论系统,大家有问题可以在这里留言 2020/4/16 更新 因为有朋友提出Disqus还是可以在翻墙后使用的,试着配置了一下,不过发现搭载后网页加载速度明显变慢,故放弃(为了一两个评论拖慢大家阅读速度感觉不是很能接受ORZ),故放弃 或许以后修改网站结构的时候会再加上一个合适的评论系统吧~ 2020/4/17更新 使用Valine制作了一个评论系统,不过还在测试阶段,如果可以的话就继续这样使用了,大家遇到问题多提意见,我好进行相应修改! 2020/9/23更新 完全重置的网站,使用了新的模板Nature,该模板增加了搜索功能,并决定使用gitalk作为评论系统(最后还是用了最初的想法,大概是设计工作中永远的痛!) 当然,留言系统依然可以使用,方便不适用gitalk或者不想登陆的朋友! 离研究生生涯越来越近,最近可能会抽时间去写几篇堆积很久的稿子,希望继续顺利,可以研究更多感兴趣的东西! ","link":"https://mbkotori.github.io/post/about/"}]}