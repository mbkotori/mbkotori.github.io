{"posts":[{"title":"换了个主题，顺便研究下在md中的latex方法","content":"在线LaTeX公式编辑器： https://www.latexlive.com/ 写法如下： $$O^{i} =x^{i}W+b$$ 这个latex方法似乎不是markdown自带的,如果要实现latex似乎需要使用到js渲染(不知道说的对不对),总之按gridea看来得在主题文件中包含,本人还没有修改主题的能力,只能将手头的主题重新尝试了下选定了一个 之后按这个顺序做了下测试,看看具体写作方法:(怪捞的) 看看$$O^{i} =x^{i}W+b$$ 看看$O^{i} =x^{i}W+b$ $$O^{i} =x^{i}W+b$$ $O^{i} =x^{i}W+b$ 在gridea的本地渲染是这样的: 下面是为了测试看主题带的渲染和gridea本地是否相同写下的四行: 看看$$O^{i} =x^{i}W+b$$ 看看Oi=xiW+bO^{i} =x^{i}W+bOi=xiW+b Oi=xiW+bO^{i} =x^{i}W+b Oi=xiW+b Oi=xiW+bO^{i} =x^{i}W+bOi=xiW+b 由此看出,双美元符号$$是行间公式,单美元符号$是行内公式 另一个有趣的是在符号之间输入\\时是会有latex关键字提示的,很方便 另一种方法,利用知乎渲染的api进行公式书写 不过这样似乎就变成html语句了,且输出公式时图片一张,不方便行内书写和格式调整 书写方法和展示结果如下: &lt;img src=&quot;https://www.zhihu.com/equation?tex=O^{i} =x^{i}W+b&quot; /&gt; ","link":"https://mbkotori.github.io/post/huan-liao-ge-zhu-ti-shun-bian-yan-jiu-xia-zai-md-zhong-de-latex-fang-fa/"},{"title":"2021/1/18","content":"pytorch搭建网络（定义模型） （1）导入torch.nn模块，nn就是利用autograd定义模型，nn的核心数据结构是Module，这是一个抽象概念，可以表示神经网络中某个词或者包含多层的神经网络。 实际运用时，最常见做法是继承nn.Module，一个nn.Module实例应该包含一些层以及返回输出的前向传播（forward）方法 这部分以后我得好好理清楚面向对象的部分 个人理解：nn.Module就是标准积木，现在我们的模型就是某个我们设计好的模型玩具，要想搭成这个玩具，我们先得从标准积木（nn.Module）中构建出我们要的指定模块(网络层)，最后forward方法就是自己搞出来的‘拼装手册’，怎么把我们的积木拼成我们的玩具。 想想8x8微型积木就好了 当然，简单方法就是nn.Sequential，这个最方便，直接net=nn.Sequential（网络层），网络层按你传入的顺序添加到计算图中 可以通过net.parameters()来查看模型所有的可学习参数，此函数将返回一个生成器。 for param in net.parameters(): print(param) 给不同子网络设置不同学习率（finetune常用） 举例： optimizer =optim.SGD([ # 如果对某个参数不指定学习率，就使用最外层的默认学习率 {'params': net.subnet1.parameters()}, # lr=0.03 {'params': net.subnet2.parameters(), 'lr': 0.01} ], lr=0.03) 似乎在pytorch中，也有类似的方法用于将多个子网络训练，之后可以进行研究 torch中的各种轮子（深度学习框架就是这么用的嘛） torch.utils.data模块提供了有关数据处理的工具，torch.nn模块定义了大量神经网络的层，torch.nn.init模块定义了各种初始化方法，torch.optim模块提供了很多常用的优化算法。 Softmax特点 softmax运算常被用在分类问题，他的好处就是将我们多分类的输出值变换成正值且和为一的概率分布 之后我们可以通过取最大概率作为分类判断依据 参考网页：https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.4_softmax-regression?id=_342-softmax回归模型 ","link":"https://mbkotori.github.io/post/2021118/"},{"title":"2021/1/16","content":"tensor和numpy转换 使用numpy()和from_numpy()相互转换,这两函数生成结果和源数据是共享内存的(速度快,但是改变一个另一个也改变) 另一方法是torch.tensor(),该方法是先数据拷贝,时间空间消耗大,也不共享内存 除了CharTensor外,所有在CPU上的tensor都可以和numpy数组转换 (感想:自由啊,回想起tensorflow中连print都打印不出内容的情况,pytorch某种程度上方便太多了!) 用to()将tensor在GPU,CPU上移动,当然你得有gpu 指定device: device = torch.device('cuda') 移动: x=x.to(device) 这就把实现了一次移动 autograd 这个包能根据输入和前向传播自动构建计算图和执行反向传播 核心类:tensor 将属性.requires_grad设置True就会追踪tensor的操作(那么就可以利用链式法则进行梯度传播),调用.backward()完成梯度计算,tensor的梯度会积累到.grad属性中 如果要取消追踪,就要调用.detach()从追踪记录分离,也可以使用with torch.no_grad()将不想追踪的代码包裹起来,这在评估模型(eval)时很有用 补充资料:PyTorch 的 backward 为什么有一个 grad_variables 参数？ https://zhuanlan.zhihu.com/p/29923090 我觉得这部分我看的是挺头晕的,这部分的逻辑还是要再深入看看的 什么是超参数 需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。 来源:https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression?id=_3-优化算法 之后有机会可以详细写一篇关于超参数的整合文章 计算代码运行时间 start = time() 应该运行的代码块 print(time() - start) python的yield关键字 起因是看到这样一段代码： 来源是：https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.2_linear-regression-scratch?id=_322-读取数据 # 本函数已保存在d2lzh包中方便以后使用 def data_iter(batch_size, features, labels): num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # 样本的读取顺序是随机的 for i in range(0, num_examples, batch_size): j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch yield features.index_select(0, j), labels.index_select(0, j) 这是一个简单的dataloader函数，但是在最后返回值上，我们可以看到他并没使用return，而是用了yield关键字 关于yield，得讨论到生成器generator，他是记录一定的算法规则，和一般的迭代器不同的是，他是在需要调用时再根据算法推算出相应元素，而不用一口气算完 yield类似return，回想一下return，函数中执行到这个关键字就会返回响应结果并终止函数执行 当yield被塞入函数里，会发生什么呢-&gt;该函数变成了生成器-&gt;我们能用next使用它或遍历 考虑到生成器的目的：记录一定算法规则得到按需生成的列表 所以yield一般在for循环语句以一定规则生成，之后调用函数，得到一个生成器，再遍历生成器使用它 当然，yield也可以用在别的地方，生成器只是一种使用方法 就像打电玩一样，你蓄力发大招的时候，如果执行了return，就直接把大招发出去了，蓄力结束 如果执行了yield，就相当于返回一个生成器对象，每调用一次next（），就发一个大招（来源知乎） 与return不同，yield在函数中返回值时会保存函数的状态，使下一次调用函数时会从上一次的状态继续执行，即从yield的下一条语句开始执行，这样做有许多好处，比如我们想要生成一个数列，若该数列的存储空间太大，而我们仅仅需要访问前面几个元素，那么yield就派上用场了，它实现了这种一边循环一边计算的机制，节省了存储空间，提高了运行效率。 总结整理我们最开始的这段代码： 我们包装的这么一个dataloader，应该将我们的数据根据需要的batchsize分成一段一段的数据（如1000数据，bs=4，就是250组），然后每次输出一组数据，这时候根据yield会保存到该组数据位置的状态，然后该组进入网络运算过程，实现网络和反向传播后，再循环回来找dataloader要下一组数据，yield的功能就体现出来了，因为我们从直接的状态就可以直接读下一组而不是从头按照索引找新的一组数据。这样 注：在pytorch中，我们的dataloader库也使用了类似这样的方法 ","link":"https://mbkotori.github.io/post/2021116/"},{"title":"2021/1/11","content":"1.pytorch各个损失函数都有自己的前提，有时候不能混用， 举例：CrossEntropyLoss，该函数进行验证时label必须是以0开始的，即 classes=【0，1，2，3】 否则报错：Assertion `cur_target &gt;= 0 &amp;&amp; cur_target &lt; n_classes’ failed. 2.数据选择转换格式时是：dtype而不是detype，注意拼写 3.多分类时使用np.argmax（），返回numpy数组中最大值的索引，根据索引即可代表网络最后给该像素的类型值 4.代码未完全确认可以使用时，优先构建小数据集来快速验证可行性，不要出现今天这种跑了4k数据才发现eval模块出错的情况 5.softmax和sigmoid： softmax用于多分类，把多个输出映射到（0，1）区间内且这些输出的结果和为1（满足概率的特性） 和上面联动：softmax后再使用argmax来找到某个概率值最大的分类索引，得到判断结果 sigmoid：注意，是sigmoid，不是sigmiod也不是sigmod，容易写错 sigmoid就是逻辑斯蒂函数，即logistic函数，把一个实数映射到（0，1）的区间，拿来做二分类，但是注意，你拿来做多分类也行，只是多分类的值相加，不会像softmax一样各分类总值相加为1，一般来说，拿来二分类还需要给他设置个阈值，将（0，1）的值映射成0/1或者0/255 另一个方法，二分类也是多分类，那你用softmax啊！但是这时候，你的网络输出应该是2层，即最后的out_channel=2，然后argmax取值作为分类，是个好办法 5.1 知乎说法： sigmoid的优点在于输出范围有限，所以数据在传递的过程中不容易发散。当然也有相应的缺点，就是饱和的时候梯度太小。sigmoid还有一个优点是输出范围为(0, 1)，所以可以用作输出层，输出表示概率。评论中 @林孟潇指出了sigmoid的另一个优点：求导容易。 作者：王赟 Maigo 链接：https://www.zhihu.com/question/24259872/answer/82598145 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 ","link":"https://mbkotori.github.io/post/2021111-zong-jie/"},{"title":"Residual Feature Aggregation Network for Image Super-Resolution","content":"翻译名:用于图像超分辨的残差特征聚合网络 abstract: 解决问题-SISR,单张图像超分辨率问题 考虑点-利用残差特征中的 残差分支-分层特征( However, existing methods neglect to fully utilize the hierarchical features on the residual branches.) 提出解决方法-提出残差特征聚合(residual feature aggregation,RFA)框架,将残差块组合在一起,并添加跳过链接[有点像densenet啊这个说法和图];提出增强空间注意力特征(enhanced spatial attention,ESA),使残差更集中于空间特征(?) 最后将RFA和ESA一同运用 introduction: SISR-直接将低分辨率LR图转成高分辨率HR图(单进单出siso),不像一般超分辨会有ms图 {读图figure1} 这里(a)实际上是一个多resblock的前向结构,每个模块包含一个resblock,由四个resblock组成 (b)中描述他们的RFA结构,先通过3个resblock前向图,他们的输出到第四个resblock,之后Res4模块输出和前三个block各自的输出concat到一块,随后1x1卷积来聚集/融合他们的特征,相对于(a)增加了1x1卷积的参数消耗 {读图figure8} 不同残差块输出的特征可以反映空间内容的不同方面 |--考虑思路:用空间注意力来提升空间信息表达能力,增强特征的空间分布 自称工作: 1.提出RFA框架 2.提出加强空间注意力ESA块 3.提出RFANet,基于RFA和ESA模块进行构建 realated work sub-pixel conv:子像素卷积,超分辨率中常用的upscale方法 https://oldpan.me/archives/upsample-convolve-efficient-sub-pixel-convolutional-layers [后续可能会出这篇文章的内容,我还没读但是看起来很有意思,他和deconv方法的不同也可以考虑进去,另一点是他能应用在SR上能不能应用在分割上,做一下考虑] {读图figure2} 基础的SR模型,卷积+多个设计好的模块+反卷积+卷积回复通道数? 基于注意力的网络模型 3.methodology 3.1 SR的简单网络结构 看作三部分: 头部:负责一个卷积层的初始特征提取,得到浅层特征F0 躯干:输入是提取到的特征F0,连接到T个basemodule的复读模式{公式2} 重建restore:将上面提取的(深层特征Ft和浅层特征F0)融合,进行放大[全局残差学习可以减轻训练难度] upsample模块是重建中的关键部分,使用子像素卷积 损失函数用L1 loss(改损失函数或许是一个很好的改进切入点) 3.2 残差特征聚合模块 残差模块包含双分支(1)残差块(2)身份块?(identity branch) 前面的残差块提取的信息很难被利用-&gt;因为它需要很长路径到达最后的卷积运算 -&gt;改进[直接将每个块的残差特征concat到一起] -&gt;concat后用1x1卷积融合他们的特征哦(这个东西可不仅仅是降维减通道) 这样的连接,不用让前面的残差信息受损或干扰-&gt;得到更具有区分性的特征表达(?) RFA可以看作一个独立模块,这也意味着我们可以把它和其他模块连接(模块复读和不同模块组合,哪个效果会好一些呢) 3.3 增强的空间注意力模块 {读图figure4} 左侧是ESAblock的形状,右侧是ESAblock中的ESA机制(mechanism) 仔细看ESA机制,对其做一个解释 ESA块放到残差块的末尾来起作用,注意力机制本质(改天可以再写一篇文章),集中体现残差特征 同时考虑到该ESA块要被连接到你后面的每个残差块,尽量不要做得太臃肿 为了完成图像SR任务,应设计一个具有较大感受野的模块(?此处没有理解.关于感受野下次可以再写一篇) 解决方法也很简单:1x1卷积降维减参数+步长卷积增大感受野 注意这里,描述的是strided conv(步长卷积)而不是dilated conv,这两者存在区别(可以详细写一下) 跨步卷积搭配池化使用#为什么/原因.可以详细描述一下 这里提到的:&quot;抛去计算量考虑,实现空间注意力块的更好方法是使用非本地模块(Non-local block)&quot;我没get到这个部分,之后要详细看一下他这里参考的两篇论文 3.4 实施细节 把RFA和ESA一同使用来构建最终的SR模块(RFANet): |-使用30个RFA模块,每个RFA包含4个ESA模块(从这里就感受到了前文为什么他们说要做一个轻便的ESA模块了) 这里的四个ESA模块也很明显:在每个resblock后,我们都给他插入一个ESA(真的有效吗....保持怀疑) 此处提到的1x1卷积的缩小率(这是个啥?),减速比? reduction ratio 3.5 讨论discussions 将文章网络和MemNet、RDN进行了讨论,描述了他们的优越性 4 实验内容 4.1 设置参数,训练等 数据集来源:DIV2K数据集 数据增强:随机旋转+翻转 batchsize=16,patch_size=48*48 数据集:Set5,Set14,B100,Urban100,Manga109 Bicubic(BI) and blur-downscale (BD) degradation models [36] are used when conducting experiments.(这部分是什么意思?没有理会到) 论文评价： 就我个人而言，这篇文章并没有太大的吸引力：RFA模块类似DenseNet结构，但是又没有很好的去解决密集连接后的参数爆炸（更有甚者他在原网络基础上又增加了ESA模块，就算通过设计使增加的参数不至于过多也是实打实地在每个模块上又添了“浓墨重彩”的一笔支出）。看似巧妙但是仔细琢磨似乎只是对网络的二度修改尝试。 另一点上，消融实验的可解释性确实不高，反而给出了一些漏洞，作者试图用实验解释自己制造的模块的作用，但是弄巧成拙，并没有取得很好的结果。 当然，作为CVPR2020录用的文章，从中我们还是能学习到一些东西的：比如文章中图表制作的合理性（我很喜欢为消融实验所做的table1和计算效率&amp;参数量的figure9，两张图既表达清晰又给人眼前一亮）；还有其中ESA模块的方法，在不能合理解释的情况下尝试他也未尝不可（作者也利用这个模块#######） ","link":"https://mbkotori.github.io/post/residual-feature-aggregation-network-for-image-super-resolution/"},{"title":"关于作者,和这个blog","content":"🏠 关于本站 本站属于个人纪录,分享遇到的坑,鉴于自己即将面对研究生生活,留下这么一个网站来作为自己的知识存储和记录也是非常重要的. 一开始考虑过CSDN or 博客园,考虑到审核问题放弃了,既然自己也没有什么特效需要,索性使用静态blog的形式来制作,也省去域名等麻烦. 👨‍💻 博客内容 主要会记录自己在深度学习方面出现的问题和遇到的坑,也可能分享一些自己的思考感悟(某种情况下可能会超出审核范围,大家辩证阅读即可). 个人更新时间不定,有兴趣的话题可以在留言区提出,有机会整理出来和大家一起分享. 📬 和我联系 本来是想使用Gittalk作为blog的评论系统,但是考虑到需要登陆github还是有点麻烦的,另外gittalk也存在一定的安全隐患.网站也需要一个图床来存图,索性使用了一个折衷的方案.(无利益相关) 网盘既可以作为图床,也有着一个简易的留言板可以作为评论区来供大家联系我,我会定期在上面进行回复,留言区限制1K条信息所以不重要的或者已解决的会删除. 这是 我们的评论系统,大家有问题可以在这里留言 2020/4/16 更新 因为有朋友提出Disqus还是可以在翻墙后使用的,试着配置了一下,不过发现搭载后网页加载速度明显变慢,故放弃(为了一两个评论拖慢大家阅读速度感觉不是很能接受ORZ),故放弃 或许以后修改网站结构的时候会再加上一个合适的评论系统吧~ 2020/4/17更新 使用Valine制作了一个评论系统,不过还在测试阶段,如果可以的话就继续这样使用了,大家遇到问题多提意见,我好进行相应修改! 2020/9/23更新 完全重置的网站,使用了新的模板Nature,该模板增加了搜索功能,并决定使用gitalk作为评论系统(最后还是用了最初的想法,大概是设计工作中永远的痛!) 当然,留言系统依然可以使用,方便不适用gitalk或者不想登陆的朋友! 离研究生生涯越来越近,最近可能会抽时间去写几篇堆积很久的稿子,希望继续顺利,可以研究更多感兴趣的东西! ","link":"https://mbkotori.github.io/post/about/"}]}